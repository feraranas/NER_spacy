{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typer\n",
    "import spacy\n",
    "import srsly\n",
    "import pandas as pd\n",
    "from spacy.tokens import DocBin, Doc, Span\n",
    "from spacy.training.example import Example\n",
    "import random\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the factory work\n",
    "from rel_pipe import make_relation_extractor, score_relations\n",
    "\n",
    "# make the config work\n",
    "from rel_model import create_relation_model, create_classification_layer, create_instances, create_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "nlp = spacy.load(\"../training/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Could not determine any instances in doc - returning doc as is.\u001b[0m\n",
      "\u001b[38;5;4mℹ Could not determine any instances in doc - returning doc as is.\u001b[0m\n",
      "\u001b[38;5;4mℹ Could not determine any instances in doc - returning doc as is.\u001b[0m\n",
      "\u001b[38;5;4mℹ Could not determine any instances in doc - returning doc as is.\u001b[0m\n",
      "\u001b[38;5;4mℹ Could not determine any instances in doc - returning doc as is.\u001b[0m\n",
      "\u001b[38;5;4mℹ Could not determine any instances in doc - returning doc as is.\u001b[0m\n",
      "\u001b[38;5;4mℹ Could not determine any instances in doc - returning doc as is.\u001b[0m\n",
      "\u001b[38;5;4mℹ Could not determine any instances in doc - returning doc as is.\u001b[0m\n",
      "\u001b[38;5;4mℹ Could not determine any instances in doc - returning doc as is.\u001b[0m\n",
      "\u001b[38;5;4mℹ Could not determine any instances in doc - returning doc as is.\u001b[0m\n",
      "\u001b[38;5;4mℹ Could not determine any instances in doc - returning doc as is.\u001b[0m\n",
      "\u001b[38;5;4mℹ Could not determine any instances in doc - returning doc as is.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load the jsonl data to predict\n",
    "data = srsly.read_jsonl(\"./golden.jsonl\")\n",
    "data_instances = [eg[\"text\"] for eg in data]\n",
    "\n",
    "# Represent data via DocBin [list of docs]\n",
    "db = DocBin()\n",
    "for text in data_instances:\n",
    "     db.add(nlp(text))\n",
    "     # doc = nlp.make_doc(text)\n",
    "     # db.add(doc)\n",
    "\n",
    "db.to_disk(\"../data/predict.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into memory\n",
    "doc_bin = DocBin(store_user_data=False).from_disk('../data/predict.spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = doc_bin.get_docs(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "for gold in docs:\n",
    "   pred = Doc(\n",
    "       nlp.vocab,\n",
    "       words=[t.text for t in gold],\n",
    "       spaces=[t.whitespace_ for t in gold],\n",
    "   )\n",
    "   pred.ents = gold.ents\n",
    "   for name, proc in nlp.pipeline:\n",
    "       pred = proc(pred)\n",
    "   examples.append(Example(pred, gold))\n",
    "   print()\n",
    "   print(f\"Text: {gold.text}\")\n",
    "   print(f\"spans: {[(e.start, e.text, e.label_) for e in pred.ents]}\")\n",
    "   for value, rel_dict in pred._.rel.items():\n",
    "       gold_labels = [k for (k, v) in gold._.rel[value].items() if v == 1.0]\n",
    "       if gold_labels:\n",
    "           print(\n",
    "               f\" pair: {value} --> gold labels: {gold_labels} --> predicted values: {rel_dict}\"\n",
    "           )\n",
    "   print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:  tok2vec \tproc:  <spacy.pipeline.tok2vec.Tok2Vec object at 0x2cb8efd70>\n",
      "name:  ner \tproc:  <spacy.pipeline.ner.EntityRecognizer object at 0x2cb8e9070>\n",
      "name:  relation_extractor \tproc:  <rel_pipe.RelationExtractor object at 0x2cb8469f0>\n"
     ]
    }
   ],
   "source": [
    "# Print the available pipeline procedures\n",
    "for name, proc in nlp.pipeline:\n",
    "     print(\"name: \", name, \"\\tproc: \", proc)\n",
    "     #    pred = proc(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.pipeline.tok2vec.Tok2Vec object at 0x2aadde6f0>\n",
      "<spacy.pipeline.ner.EntityRecognizer object at 0x2aadbdf50>\n",
      "<rel_pipe.RelationExtractor object at 0x2aaddfa70>\n"
     ]
    }
   ],
   "source": [
    "# Example script to retrieve a specific pipeline procedure\n",
    "tok2vec = nlp.get_pipe(\"tok2vec\")\n",
    "# transformer = nlp.get_pipe(\"transformer\")\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "relation_extractor = nlp.get_pipe(\"relation_extractor\")\n",
    "print(tok2vec)\n",
    "print(ner)\n",
    "print(relation_extractor)\n",
    "# print(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function create_instances.<locals>.<lambda> at 0x2aadb3c40>\n"
     ]
    }
   ],
   "source": [
    "# Example script to retrieve a function from a pipeline procedure\n",
    "get_instances = relation_extractor.model.attrs[\"get_instances\"]\n",
    "print(get_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on a document without modifying it\n",
    "doc1 = nlp(\"The Associate Security Analyst supports security systems, operations administration, monitoring and maintenance of cyber security systems and applications. He/She monitors security alerts and events. He collects and documents information based on established practices and supports the preparation and publishing of security advisories. He assists with the analysis of security-related information and events, escalation of incidents for validation and remediation. He is required to be on standby with on-call availability with varied shifts including nights, weekends and holidays. He is familiar with cyber security standards, protocols and frameworks, and is required to act in accordance with the Cyber Security Act 2018. He is knowledgeable in using various cyber security tools and techniques to monitor and resolve incidents. The Associate Security Analyst is alert and vigilant in performing monitoring activities and is able to analyse and resolve security-related issues critically. He communicates clearly in his interactions with others and coordinates effectively with his team to perform security operations.\")\n",
    "doc2 = nlp(\"The Chief Information Security Officer develops and drives the vision for the information security function. He/She acts as the authority for the development and enforcement of organisation security strategy, standards and policies, and has ultimate responsibility for ensuring the protection of corporate information. He guides the design and continuous improvement of the IT security architecture and Cyber Risk Maturity Model that balances business needs with security risks. He advises the board and top executives on all security matters and sets directions for complying with regulatory inquiries, legal and compliance regulations, inspections and audits. He is an expert in cyber security compliance standards, protocols and frameworks, as well as the Cyber Security Act 2018. He is keeps abreast of cyber-related applications and hardware technologies and services, and is constantly on the look-out for new technologies that may be leveraged on to enhance work processes, or which may pose as potential threats. The Chief Information Security Officer is an inspirational and influential leader, who displays sound judgement and decisiveness in ensuring that corporate information is well protected and secured. He is strategic in his approach toward resource management and capability development among his teams.\")\n",
    "doc3 = nlp(\"The .NET Framework is a software framework developed by Microsoft that runs primarily on Microsoft Windows. It includes a large class library called Framework Class Library (FCL) and provides language interoperability across several programming languages. Programs written for .NET Framework execute in a software environment named the Common Language Runtime (CLR). The CLR is an application virtual machine that provides services such as security, memory management, and exception handling. As such, computer code written using .NET Framework is called \\\"managed code\\\". FCL and CLR together constitute the .NET Framework.\")\n",
    "doc4 = nlp(\"\\nDefined by Microsoft for use in recent versions of Windows, an assembly in the Common Language Infrastructure (CLI) is a compiled code library used for deployment, versioning, and security. There are two types: process assemblies (EXE) and library assemblies (DLL). A process assembly represents a process that will use classes defined in library assemblies. CLI assemblies contain code in CIL, which is usually generated from a CLI language, and then compiled into machine language at run time by the just-in-time compiler. In the .NET Framework implementation, this compiler is part of the Common Language Runtime (CLR).\")\n",
    "doc5 = nlp(\"10 Gigabit Ethernet is a group of computer networking technologies for transmitting Ethernet frames at a rate of 10 gigabits per second. It was first defined by the IEEE 802.3ae-2002 standard. Unlike previous Ethernet standards, 10 Gigabit Ethernet defines only full-duplex point-to-point links which are generally connected by network switches; shared-medium CSMA/CD operation has not been carried over from the previous generations Ethernet standards so half-duplex operation and repeater hubs do not exist in 10GbE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spans: [(1, 'Gigabit Ethernet is', 'SKILL'), (7, 'computer networking technologies', 'SKILL'), (12, 'Ethernet frames', 'SKILL'), (35, 'Ethernet standards', 'SKILL'), (39, 'Gigabit Ethernet defines', 'SKILL'), (57, 'network switches', 'SKILL'), (76, 'Ethernet standards', 'SKILL')]\n"
     ]
    }
   ],
   "source": [
    "print(f\"spans: {[(e.start, e.text, e.label_) for e in doc5.ents]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gigabit Ethernet is SKILL\n",
      "computer networking technologies SKILL\n",
      "Ethernet frames SKILL\n",
      "Ethernet standards SKILL\n",
      "Gigabit Ethernet defines SKILL\n",
      "network switches SKILL\n",
      "Ethernet standards SKILL\n"
     ]
    }
   ],
   "source": [
    "for e in doc5.ents:\n",
    "     print(e, e.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in doc5._.rel.items():\n",
    "     if e[1]['KNOWS'] > 0.5:\n",
    "          print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spans: [(10, 'Windows,', 'SKILL'), (17, 'Language Infrastructure', 'OCC'), (34, 'security. There are', 'SKILL'), (73, 'CIL,', 'OCC'), (82, 'language,', 'SKILL'), (88, 'machine language', 'SKILL'), (115, 'Language Runtime (CLR).', 'OCC')]\n"
     ]
    }
   ],
   "source": [
    "print(f\"spans: {[(e.start, e.text, e.label_) for e in doc4.ents]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.training.example import Example\n",
    "\n",
    "pred = Doc(\n",
    "    nlp.vocab,\n",
    "    words=[t.text for t in doc4],\n",
    "    spaces=[t.whitespace_ for t in doc4],\n",
    ")\n",
    "pred.ents = doc4.ents\n",
    "for name, proc in nlp.pipeline:\n",
    "    pred = proc(pred)\n",
    "\n",
    "example = Example(pred, doc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.031369336\n",
      "0.011399968\n"
     ]
    }
   ],
   "source": [
    "# print(doc3.ents)\n",
    "# doc4._.rel\n",
    "\n",
    "# for value, rel_dict in doc4._.rel.items():\n",
    "#      print(rel_dict)\n",
    "\n",
    "for (k, v) in doc4._.rel[(10, 17)].items():\n",
    "     print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go a step back an add manual instances to doc1 & doc2\n",
    "# doc1.ents = [\n",
    "#     Span(doc1, 1, 4, label=\"OCCUPATION\"),\n",
    "#     Span(doc1, 5, 7, label=\"SKILL\"),\n",
    "#     Span(doc1, 8, 10, label=\"SKILL\"),\n",
    "#     Span(doc1, 15, 17, label=\"SKILL\"),\n",
    "# ]\n",
    "\n",
    "# doc2.ents = [\n",
    "#     Span(doc1, 1, 5, label=\"OCCUPATION\"),\n",
    "#     Span(doc1, 12, 14, label=\"SKILL\"),\n",
    "#     # Span(doc1, 28, 30, label=\"SKILL\"),\n",
    "#     # Span(doc1, 43, 45, label=\"SKILL\"),\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = relation_extractor.predict([doc1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.98137116e-01, 1.13621921e-08],\n",
       "       [9.97541308e-01, 9.77544889e-09],\n",
       "       [9.98295128e-01, 1.47525085e-08],\n",
       "       [4.52181703e-05, 3.71496078e-08],\n",
       "       [2.50027166e-03, 2.90826185e-09],\n",
       "       [3.60449357e-03, 4.38897052e-09],\n",
       "       [3.86377360e-05, 4.11782395e-08],\n",
       "       [2.82057608e-03, 3.74691034e-09],\n",
       "       [3.08154104e-03, 4.86491647e-09],\n",
       "       [3.50350128e-05, 4.80279638e-08],\n",
       "       [2.55823880e-03, 4.37018377e-09],\n",
       "       [1.93827704e-03, 3.75987552e-09]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 5): {'KNOWS': 0.9981371, 'HAS': 1.1362192e-08}}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_instances = relation_extractor.model.attrs[\"get_instances\"]\n",
    "\n",
    "# for doc in docs:\n",
    "# for (e1, e2) in get_instances(doc1):\n",
    "#      print(e1.start, \", \", e2.start)\n",
    "# relation_extractor.set_annotations([doc1], scores)\n",
    "doc1._.rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_extractor.add_label(\"OCCUPATION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Associate Security Analyst supports security systems, operations administration, monitoring and maintenance of cyber security systems and applications. He/She monitors security alerts and events. He collects and documents information based on established practices and supports the preparation and publishing of security advisories. He assists with the analysis of security-related information and events, escalation of incidents for validation and remediation. He is required to be on standby with on-call availability with varied shifts including nights, weekends and holidays. He is familiar with cyber security standards, protocols and frameworks, and is required to act in accordance with the Cyber Security Act 2018. He is knowledgeable in using various cyber security tools and techniques to monitor and resolve incidents. The Associate Security Analyst is alert and vigilant in performing monitoring activities and is able to analyse and resolve security-related issues critically. He communicates clearly in his interactions with others and coordinates effectively with his team to perform security operations.', 'The Chief Information Security Officer develops and drives the vision for the information security function. He/She acts as the authority for the development and enforcement of organisation security strategy, standards and policies, and has ultimate responsibility for ensuring the protection of corporate information. He guides the design and continuous improvement of the IT security architecture and Cyber Risk Maturity Model that balances business needs with security risks. He advises the board and top executives on all security matters and sets directions for complying with regulatory inquiries, legal and compliance regulations, inspections and audits. He is an expert in cyber security compliance standards, protocols and frameworks, as well as the Cyber Security Act 2018. He is keeps abreast of cyber-related applications and hardware technologies and services, and is constantly on the look-out for new technologies that may be leveraged on to enhance work processes, or which may pose as potential threats. The Chief Information Security Officer is an inspirational and influential leader, who displays sound judgement and decisiveness in ensuring that corporate information is well protected and secured. He is strategic in his approach toward resource management and capability development among his teams.', \"The Cyber Risk Analyst conducts cyber risk assessment in support of technology initiatives to help identify IT related risk and determines appropriate controls to mitigate risks. He/She monitors, tracks and manages risk mitigations and exceptions to ensure cyber security standards and policies are established. He applies a defined set of analytical or scientific methods and works independently. He is also responsible for documentation of cyber risk assessment reports. He is familiar with cyber security standards, protocols and frameworks, and acts in accordance with the Cyber Security Act 2018. He is knowledgeable in using various cyber security monitoring and analysis tools and techniques depending on the organisation's needs and requirements. The Cyber Risk Analyst is vigilant and systematic in identifying cyber risks and enjoys analysing and investigating such issues. He is a strong team player, and communicates well both verbally and in writing.\", \"The Cyber Risk Manager guides the assessment of information and cyber risks associated with technology initiatives and provides recommendations on control requirements by risk policy and standards. He/She manages and coordinates responses to regulatory inquiries, inspections, audits and ensures cyber security standards and policies are established and implemented. He oversees the development of reports and implements policies and standards. He manages employees and is held accountable for the performance and results of a team. He provides guidance on security measures and protocols to stakeholders. He is familiar with cyber security standards, protocols and frameworks, and ensures the organisations compliance to the Cyber Security Act 2018. He is knowledgeable in using various cyber security monitoring and analysis tools and techniques depending on the organisation's needs and requirements. He also has expertise in cyber risk mitigation strategies and protocols. The Cyber Risk Manager has a sharp, analytical mind and is able to anticipate problems and risks to mitigate them ahead of time. He is an excellent communicator who works well with others and promotes a cooperative working environment and relationships within and beyond his team.\", 'The Forensics Investigation Manager plans and oversees the investigation processes and protocols after a cyber-threat or incident. He/She is responsible to ensure that the data is collected and analysed properly. He is also responsible for developing a forensics investigation strategy and overseeing the forensics investigations to ensure the threat is classified and future actions are recommended to the affected stakeholders. He is familiar with different types of threats, cyber security standards, protocols and frameworks, and ensures the organisations compliance with the Cyber Security Act 2018. He is knowledgeable of hardware and software applications to analyse threat data from various sources. The Forensics Investigation Manager is diligent and watchful in the investigation activities. He is also a confident leader who develops plans and solutions to address security incidents, and has a passion for engaging and developing others in his team.']\n"
     ]
    }
   ],
   "source": [
    "# Load the jsonl data to predict\n",
    "data = srsly.read_jsonl(\"./golden.jsonl\")\n",
    "data_instances = [eg[\"text\"] for eg in data]\n",
    "print(data_instances[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added  138  documents\n"
     ]
    }
   ],
   "source": [
    "# Represent data via DocBin [list of docs]\n",
    "db = DocBin()\n",
    "for text in data_instances:\n",
    "     doc = nlp.make_doc(text)\n",
    "     db.add(doc)\n",
    "\n",
    "print(\"Added \", len(db), \" documents\")\n",
    "\n",
    "db.to_disk(\"predict.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into memory\n",
    "doc_bin = DocBin(store_user_data=True).from_disk('./predict.spacy')\n",
    "# docs = doc_bin.get_docs(nlp.vocab)\n",
    "docs = db.get_docs(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "     print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _score_and_format(examples, thresholds):\n",
    "    for threshold in thresholds:\n",
    "        r = score_relations(examples, threshold)\n",
    "        results = {k: \"{:.2f}\".format(v * 100) for k, v in r.items()}\n",
    "        print(f\"threshold {'{:.2f}'.format(threshold)} \\t {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for entities\n",
    "examples = []\n",
    "random_examples = []\n",
    "for gold in docs:\n",
    "    pred = Doc(\n",
    "        nlp.vocab,\n",
    "        words=[t.text for t in gold],\n",
    "        spaces=[t.whitespace_ for t in gold],\n",
    "    )\n",
    "    pred.ents = gold.ents\n",
    "\n",
    "    for name, proc in nlp.pipeline:\n",
    "        pred = proc(pred)\n",
    "    examples.append(Example(pred, gold))\n",
    "\n",
    "    relation_extractor = nlp.get_pipe(\"relation_extractor\")\n",
    "    get_instances = relation_extractor.model.attrs[\"get_instances\"]\n",
    "    for (e1, e2) in get_instances(pred):\n",
    "        offset = (e1.start, e2.start)\n",
    "        if offset not in pred._.rel:\n",
    "            pred._.rel[offset] = {}\n",
    "        for label in relation_extractor.labels:\n",
    "            pred._.rel[offset][label] = random.uniform(0, 1)\n",
    "    random_examples.append(Example(pred, gold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is Shaka Khan? {(7, 17, 'PERSON')}\n",
      "I like London and Berlin. {(18, 24, 'LOC'), (7, 13, 'LOC')}\n",
      "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'sents_p': None, 'sents_r': None, 'sents_f': None, 'tag_acc': None, 'pos_acc': None, 'morph_acc': None, 'morph_micro_p': None, 'morph_micro_r': None, 'morph_micro_f': None, 'morph_per_feat': None, 'dep_uas': None, 'dep_las': None, 'dep_las_per_type': None, 'ents_p': None, 'ents_r': None, 'ents_f': None, 'ents_per_type': None, 'cats_score': 0.0, 'cats_score_desc': 'macro F', 'cats_micro_p': 0.0, 'cats_micro_r': 0.0, 'cats_micro_f': 0.0, 'cats_macro_p': 0.0, 'cats_macro_r': 0.0, 'cats_macro_f': 0.0, 'cats_macro_auc': 0.0, 'cats_f_per_type': {}, 'cats_auc_per_type': {}}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.tokens import Doc\n",
    "from spacy.training.example import Example\n",
    "\n",
    "examples = [\n",
    "    ('Who is Shaka Khan?',\n",
    "     {(7, 17, 'PERSON')}),\n",
    "    ('I like London and Berlin.',\n",
    "     {(7, 13, 'LOC'), (18, 24, 'LOC')})\n",
    "]\n",
    "\n",
    "def evaluate(ner_model, examples):\n",
    "    scorer = Scorer()\n",
    "    example = []\n",
    "    for input_, annot in examples:\n",
    "        pred = ner_model(input_)\n",
    "        print(pred,annot)\n",
    "        temp = Example.from_dict(pred, dict.fromkeys(annot))\n",
    "        example.append(temp)\n",
    "    scores = scorer.score(example)\n",
    "    return scores\n",
    "\n",
    "ner_model = spacy.load('en_core_web_md') # for spaCy's pretrained use 'en_core_web_sm'\n",
    "results = evaluate(ner_model, examples)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
